<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Tim Menzies timm@ieee.org" />
  <title>A Little Less AI (tiny models, more power)</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {  background-color: #f8f8f8; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ef2929; } /* Alert */
    code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #204a87; } /* Attribute */
    code span.bn { color: #0000cf; } /* BaseN */
    code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4e9a06; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #8f5902; font-style: italic; } /* Comment */
    code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
    code span.dt { color: #204a87; } /* DataType */
    code span.dv { color: #0000cf; } /* DecVal */
    code span.er { color: #a40000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #0000cf; } /* Float */
    code span.fu { color: #204a87; font-weight: bold; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
    code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
    code span.ot { color: #8f5902; } /* Other */
    code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
    code span.sc { color: #ce5c00; font-weight: bold; } /* SpecialChar */
    code span.ss { color: #4e9a06; } /* SpecialString */
    code span.st { color: #4e9a06; } /* String */
    code span.va { color: #000000; } /* Variable */
    code span.vs { color: #4e9a06; } /* VerbatimString */
    code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
   pre {font-size: x-small;} </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">A Little Less AI (tiny models, more power)</h1>
<p class="author">Tim Menzies<br>timm@ieee.org</p>
<p class="date">July, 2025</p>
</header>
<table style="width:76%;">
<colgroup>
<col style="width: 76%" />
</colgroup>
<tbody>
<tr>
<td><em>“The best thing to do with most data is throw it away.”</em>
–me</td>
</tr>
<tr>
<td><em>“YAGNI”</em> (you aren’t gonna need it) –Kent Beck</td>
</tr>
<tr>
<td><em>“Less, but better.”</em> –Dieter Rams</td>
</tr>
<tr>
<td><em>“Subtract.”</em> –Leidy Klotz</td>
</tr>
</tbody>
</table>
<p>To say the least, everyone is focused on big AI right that
assumes</p>
<p>What should we be teaching newcomers to software engineering? Talking
with fellow instructors, we’ve noticed two growing challenges. First,
students often lack practice in analyzing and critiquing code.
Post-COVID learners, raised on search engines and chatbots, are great at
vibing with code—copying, tweaking, prompting—but less adept at
understanding what makes it tick. Second, the systems they encounter
often feel too large or opaque to examine. As Leidy Klotz notes, people
systematically overlook subtractive changes—we keep adding, even when
taking away would serve us better. This leads to complexity and bloat.
In teaching, it overwhelms. Such complexity means that code is something
that appears on a screen, not something they can explore and own and
shape.</p>
<p>These challenges are connected. Simpler systems invite deeper
engagement and more ownership. D. Richard Hipp, creator of SQLite, says
“If you want to be free, that means doing things yourself.”<a
href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a> He likens it to backpacking—carrying
only what you need to move freely. You do not need to build Microsoft
Windows; you need just enough to do something meaningful. This book
shows how powerful results can emerge from surprisingly little code and
data.</p>
<p>That ethic—simplicity, self-reliance, deep understanding—is fading.
For many, that’s fine; it means producing more code, more quickly. But
for engineers tasked with building reliable, high-quality systems under
real-world constraints, that loss matters. We must teach what happens
inside the code, especially the complex systems that power today’s AI.
As these tools proliferate, we need code surgeons who can diagnose,
intervene, and improve what lies beneath the surface. We need to reclaim
authorship of our code, encouraging understanding and fostering a
mindset of critique rather than cargo cult reuse.</p>
<p>This reluctance to look inside has serious consequences. Chief among
them is the crisis of <em>reproducibility</em>: big AI experiments are
difficult to replicate, and comparative evaluations are rare. For
example, a recent review<a href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a> of 229 software engineering papers
using LLMs (a big AI technique) found that only 13 (just 5%) compared
their results to any alternative. This lack of introspection is not only
methodologically flawed; it also suppresses innovation. As seen in the
<em><a href="#examples">Example</a></em> section, simpler approaches can
often perform just as well—or better—while being faster, more
transparent, and easier to critique_<a href="#fn3" class="footnote-ref"
id="fnref3" role="doc-noteref"><sup>3</sup></a> <a href="#fn4"
class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> <a
href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a> <a href="#fn6" class="footnote-ref"
id="fnref6" role="doc-noteref"><sup>6</sup></a> <a href="#fn7"
class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> <a
href="#fn8" class="footnote-ref" id="fnref8"
role="doc-noteref"><sup>8</sup></a> <a href="#fn9" class="footnote-ref"
id="fnref9" role="doc-noteref"><sup>9</sup></a>.</p>
<p>So what are those alternatives? “Less AI” offers one path forward.
Where big AI emphasizes volume, less AI assumes that useful models are
tiny gems obscured by irrelevant or noisy or redundant data. Finding
useful models is a hence a progress of pruning anything that is
superfluous or confusing. To say that another way:</p>
<blockquote>
<p><strong>Menzies’s 4th law:</strong> The best thing to do with most
data is throw it away<a href="#fn10" class="footnote-ref" id="fnref10"
role="doc-noteref"><sup>10</sup></a>.</p>
</blockquote>
<p><strong>EZR</strong> is a compact implementation of this law. It is
an incremental <em>active learner</em><a href="#fn11"
class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>,
which means “what to do next” is determined from “what has been seen so
far”. By peeking at the data before processing it, tools like
<strong>EZR</strong> can avoid irrelevancies, redundancies, and noisy
data. In this way, models can be built, very quickly, from very little
data. In just a few hundred lines of code, EZR supports a wide range of
core AI tasks—classification, clustering, regression, active learning,
multi-objective optimization, and explainability.</p>
<p>This research note introduces EZR and explains its design,
motivation, and implications. After a code walk-through, we evaluate EZR
using over 100 diverse examples from the <a
href="https://github.com/timm/moot">MOOT repository</a><a href="#fn12"
class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>,
which captures problems from software engineering optimization, such as
tuning analytic tools, adjusting configuration parameters, and guiding
software process decisions.</p>
<p>The MOOT benchmarks help answer several core questions about EZR:</p>
<ul>
<li><p><strong>RQ1: Is it simple?</strong><br />
Yes- EZR has a compact, readable codebase, ideal for teaching and
tinkering.</p></li>
<li><p><strong>RQ2: Is it fast?</strong><br />
Yes— EZR completes tasks in milliseconds that take hours for big
AI.</p></li>
<li><p><strong>RQ3: Is it effective?</strong><br />
Yes— MOOT’s active learners achieve near-optimal results after seeing
just a few dozen examples.</p></li>
<li><p><strong>RQ4: Is it insightful?</strong><br />
Yes— EZR reveals a perspective on learning that encourages explanation
and critical analysis, not just automation.</p></li>
<li><p><strong>RQ5: Is it general?</strong> Within the scope of
MOOT-style optimization of SE tasks, yes. While not designed for text
generation (you’ll still need LLMs for that), EZR excels at fast model
building and external critique—an essential capability when teaching
students to open up and reason about AI systems.</p></li>
</ul>
<h2 id="installation">Installation</h2>
<p>To run the examples of this book:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>mkdir demo<span class="op">;</span> cd demo</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>git clone https:<span class="op">//</span>github.com<span class="op">/</span>timm<span class="op">/</span>moot <span class="co"># &lt;== data</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>git clone https:<span class="op">//</span>github.com<span class="op">/</span>timm<span class="op">/</span>ezr  <span class="co"># &lt;== code</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>cd ezr</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>python3 <span class="op">-</span>B ezr.py <span class="op">-</span>f ..<span class="op">/</span>moot<span class="op">/</span>optimize<span class="op">/</span>config<span class="op">/</span>SS<span class="op">-</span>M.csv </span></code></pre></div>
<h2 id="a-quick-example">A Quick Example</h2>
<p>Just to give all this a little context, here’s a quick example to get
us going.</p>
<p>Say we want to configure a database to reduce energy use, runtime,
and CPU load. The database exposes dozens of tuning options—storage,
logging, locking, compression, encryption, and more. Understanding how
each setting impacts performance is daunting.</p>
<p>Imagine we have a log of 800+ configurations, each showing binary
control options and their effects. Some settings are “best” and lead to
low energy, runtime and cpu usage; e.g. </p>
<pre><code>choices                     Effects
----------------            -----------------------
control settings →            Energy-, time-,  cpu-
0,0,0,0,1,0,...               6.6,    248.4,   2.1   ← best
1,1,0,1,1,1,...              16.8,    518.6,  14.1   ← rest
...</code></pre>
<p>Given such a log, any number of AI tools could learn a model for what
predicts for “best” and what avoids “rest”. But here’s the problem: most
real-world scenarios don’t come with complete logs. Labeling each
configuration (e.g., by running benchmarks or consulting some human
expert) is expensive, slow, and (sometimes) even impossible. So how can
we learn anything useful, with least effort (i.e. after asking for
fewest labels).</p>
<p>That’s where <strong>EZR</strong> comes in. It uses a minimalist
<strong>A-B-C</strong> strategy:</p>
<ul>
<li><p><strong>A: Ask anything</strong><br />
Randomly sample a few rows (e.g., <em>A = 4</em>) and label them to seed
the process. In this seed, we build two models (one for “best” and one
for “rest”). For unballed rows, this models can report the likelihood
that some new example belongs to “best” or “rest” (these are called
<em>b,r</em>).</p></li>
<li><p><strong>B: Build a model</strong><br />
Iteratively label up to <em>B = 24</em> additional rows. Each new row is
selected based on its potential to improve the model (specifically, we
look for things that maximize <span
class="math inline"><em>b</em>/<em>r</em></span>).</p></li>
<li><p><strong>C: Check the model</strong><br />
Apply the model to all unlabeled rows, then evaluate just a few (e.g.,
<em>C = 5</em>) of the most promising ones.</p></li>
</ul>
<p>In this example, after labeling just 24 out of 800 rows (∼4%), EZR
constructs a decision tree with interpretable rules. One path to a
near-optimal configuration is:</p>
<pre><code>if crypt_blowfish == 0 and 
   memory_tables == 1 and 
   small_log == 0 and 
   logging == 0 and 
   txc_mvlocks == 0 and 
   no_write_delay == 0
then win ≈ 99%</code></pre>
<p>In the above “win” is a measure of how close we get to optimum. A
“win” of zero means we have not changed anything and a “win” of 100
means we are found ways to select for the best values. This branch
achieves a 99% “win” so it very nearly perfect.</p>
<p>To test this model, EZR applies it to all 800 rows and selects the
top <em>C = 5</em> rows it predicts to be best. The actual measured
performance of those five rows confirms the model’s judgment: the top
pick is within 2% of the global best.</p>
<p>All this was achieved with only a few dozen queries, processed by
just a few hundred lines of code.<br />
Think of it as the Pareto principle on steroids. <strong>Vilfredo
Pareto</strong> proposed that 80% of the gain often comes from just 20%
of the work — and throughout the history of AI, many analogous results
reinforce this idea:</p>
<p>The history of “Less AI”:</p>
<table>
<colgroup>
<col style="width: 6%" />
<col style="width: 17%" />
<col style="width: 15%" />
<col style="width: 60%" />
</colgroup>
<thead>
<tr>
<th>Year</th>
<th>What</th>
<th>Who / System</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>1902</td>
<td>PCA</td>
<td>Pearson</td>
<td>Large datasets projected to a few components.</td>
</tr>
<tr>
<td>1960s</td>
<td>Narrows</td>
<td>Amarel <a href="#fn13" class="footnote-ref" id="fnref13"
role="doc-noteref"><sup>13</sup></a></td>
<td>Chess search guided by tiny sets of key variable settings.</td>
</tr>
<tr>
<td>1980s</td>
<td>ATMS</td>
<td>de Kleer</td>
<td>Diagnoses focus only on assumptions independent of others.</td>
</tr>
<tr>
<td>1984</td>
<td>Distance-Preseration</td>
<td>Johnson &amp; Lindenstrauss <a href="#fn14" class="footnote-ref"
id="fnref14" role="doc-noteref"><sup>14</sup></a></td>
<td>High-dimensional data can be embedded in low dimensions while
preserving pairwise distances.</td>
</tr>
<tr>
<td>1996</td>
<td>ISAMP</td>
<td>Crawford &amp; Baker <a href="#fn15" class="footnote-ref"
id="fnref15" role="doc-noteref"><sup>15</sup></a></td>
<td>Fast random retries outperform exhaustive backtracking.</td>
</tr>
<tr>
<td>1997</td>
<td>Feature Subset Selection</td>
<td>John &amp; Kohavi <a href="#fn16" class="footnote-ref" id="fnref16"
role="doc-noteref"><sup>16</sup></a></td>
<td>Up to 80% of features can be ignored without hurting accuracy.</td>
</tr>
<tr>
<td>2002</td>
<td>Backdoors</td>
<td>Williams et al. <a href="#fn17" class="footnote-ref" id="fnref17"
role="doc-noteref"><sup>17</sup></a></td>
<td>Setting a few variables reduces exponential runtimes to
polynomial.</td>
</tr>
<tr>
<td>2005</td>
<td>Semi-Supervised Learning</td>
<td>Zhou et al. <a href="#fn18" class="footnote-ref" id="fnref18"
role="doc-noteref"><sup>18</sup></a></td>
<td>Data often lies on low-dimensional manifolds inside high-dimensional
spaces.</td>
</tr>
<tr>
<td>2008</td>
<td>Exemplars</td>
<td>Menzies <a href="#fn19" class="footnote-ref" id="fnref19"
role="doc-noteref"><sup>19</sup></a></td>
<td>Small samples can summarize and model large datasets.</td>
</tr>
<tr>
<td>2009</td>
<td>Active Learning</td>
<td>Settles <a href="#fn20" class="footnote-ref" id="fnref20"
role="doc-noteref"><sup>20</sup></a></td>
<td>Selectively query the most informative examples. <br>Unlike
semi-supervised methods, the learner actively shapes its training
set.</td>
</tr>
<tr>
<td>2005–20</td>
<td>Key Vars in SE</td>
<td>Menzies et al.</td>
<td>Dozens of SE models controlled by just a few parameters.</td>
</tr>
<tr>
<td>2010+</td>
<td>Surrogate Models</td>
<td>Various</td>
<td>Optimizers can be approximated from small training sets.</td>
</tr>
<tr>
<td>2020s</td>
<td>Model Distillation</td>
<td>Various</td>
<td>Large AI models reduced in size by orders of magnitude with little
performance loss.</td>
</tr>
</tbody>
</table>
<hr />
<p>Inspired by all this, I once write a quick and dirty demonstrator
while teaching a graduate class The resulting code (EZR, version 0.1)
peeked at a random sample of the data to learn two tiny models
<em>best</em> and <em>rest</em> models. It performed startlingly well.
That led to several papers comparing EZR’s minimalist approach to
state-of-the-art optimizers. In all cases, <strong>EZR’s “less AI”
performed just as well as the more complex systems.</strong></p>
<p><strong>Long story short</strong>: with EZR, as with much of
intelligent system design, <strong>success does not come from reasoning
about everything</strong>, but from identifying and leveraging just the
<strong>right few things</strong>. This idea is widely known but rarely
applied. In most cases, new problems are still tackled with overly
complex, heavyweight AI tools. Perhaps we need to change that</p>
<h2 id="discussion-why-this-works">Discussion: Why This Works</h2>
<h3 id="why-not-label-everything">Why not label everything?</h3>
<p>Because labeling is expensive:</p>
<ul>
<li>Some benchmarks are slow to run.</li>
<li>Some require rebuilding complex systems.</li>
<li>Manual annotation is costly and error-prone.</li>
</ul>
<p>Prior work shows that labeling large datasets can take years<a
href="#fn21" class="footnote-ref" id="fnref21"
role="doc-noteref"><sup>21</sup></a><a href="#fn22" class="footnote-ref"
id="fnref22" role="doc-noteref"><sup>22</sup></a><a href="#fn23"
class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>.
Even using big AI and large language models has limitations: they help
only in narrow, well-structured tasks and still require careful
deployment<a href="#fn24" class="footnote-ref" id="fnref24"
role="doc-noteref"><sup>24</sup></a>.</p>
<h3 id="so-how-does-ezr-help">So how does EZR help?</h3>
<p>The tiny AI assumption is that models are tiny gems obscured by much
irrelevant or noisy or redundant data. Internally, EZR is an
<strong>contrastive active learner</strong>. By focusing on examples
with large <span class="math inline"><em>b</em>/<em>r</em></span> score,
EZR only processes a handful of examples that most distinguish good from
bad. It labels only the most informative rows (and updates in models
from that label). As a side-effect, this also dodges superfluous or
confusing data. In this way, EZR we can very quickly build very
effective models.</p>
<p>To say all that another way, you do not need a mountain of
information—just the right few examples.</p>
<h3 id="why-use-a-decision-tree">Why use a decision tree?</h3>
<p>Doing, without learning, means you are doomed to doing it all again,
every time that need arises. But if you can learn some generalization,
then the next time something comes up, you already know how to handle
it. According the physicist Enrst Rutherford, your explainations should
be as simple as possible.</p>
<div class="line-block">Enrst Rtherford: A theory that you can’t explain
to a bartender is probably no damn good</div>
<p>Decision trees learned from <span class="math inline">$_B=24_$</span>
examples are:</p>
<ul>
<li>Fast to learn<br />
</li>
<li>Interpretable<br />
</li>
<li>Naturally sparse</li>
</ul>
<p>Each node splits examples based on a binary feature. Leaves group
similar configurations. A single path in the tree becomes a recipe for
improvement.</p>
<h3 id="how-is-win-defined">How is “win” defined?</h3>
<p>EZR uses a normalized utility score:</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>win <span class="op">=</span> <span class="dv">100</span> × (<span class="dv">1</span> <span class="op">-</span> (x <span class="op">-</span> best) <span class="op">/</span> (median <span class="op">-</span> best))</span></code></pre></div>
<p>Here <code>x</code> is the performance of a configuration,
<code>best</code> is the global best, and <code>median</code> is a
typical value.</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> A win of <span class="op">**</span><span class="dv">100</span><span class="op">**</span> means matching the best.  </span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> A win of <span class="op">**</span><span class="dv">0</span><span class="op">**</span> means average.  </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Negative wins mean regression.</span></code></pre></div>
<h3 id="how-general-is-this">How General is This?</h3>
<p>For years, we have been collecting what are known as “search-based
SE” problems into the MOOT repository (MOOT= multi-objective
optimization tests). The above case study is one of over 110 data sets
in MOOT. As shown by the following table, MOOT data can have up tp has
100,000 rows, over 1000 choices, and up to 3 effects. More usually, MOOT
data has 10,000 rows,9 choices and 3 effects.</p>
<table>
<thead>
<tr>
<th style="text-align: right;">percentile</th>
<th style="text-align: right;">25</th>
<th style="text-align: right;">50</th>
<th style="text-align: right;">75</th>
<th style="text-align: right;">100</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">#rows</td>
<td style="text-align: right;">1023</td>
<td style="text-align: right;">10,000</td>
<td style="text-align: right;">10,000</td>
<td style="text-align: right;">100,000</td>
</tr>
<tr>
<td style="text-align: right;">#choices</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">1,044</td>
</tr>
<tr>
<td style="text-align: right;">#effects</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">3</td>
</tr>
</tbody>
</table>
<p>To assess EZR on all that data, ten times, we divided each data set
into a train and test set (50:50). Next:</p>
<ul>
<li>A tree was built by EZR on the training data;</li>
<li>Test rows were sorted using the tree’s predictions;</li>
<li>The top <em>C=5</em> predictions were then labelled and the win of
the best row was printed.</li>
<li>Just as a prudence check, this was compared against dumb guessing.
<em>C=5</em> rows were picked random from the test set, sorted them by
their labels, and win of the best row (selected by this dumb method) was
printed.</li>
</ul>
<p>For this experiment, we used the same control parameters as seen
above; i.e. <em>A,B,C</em>=4,24,5. Across these 10 experiments with 110
case studies, EZR usually found a “win” of 70%.</p>
<table>
<thead>
<tr>
<th style="text-align: right;">percentile</th>
<th style="text-align: right;">EZR</th>
<th style="text-align: right;">delta = EZR - dumb</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">10</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">-12</td>
</tr>
<tr>
<td style="text-align: right;">20</td>
<td style="text-align: right;">35</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: right;">30</td>
<td style="text-align: right;">42</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: right;">40</td>
<td style="text-align: right;">59</td>
<td style="text-align: right;">3</td>
</tr>
<tr>
<td style="text-align: right;">50</td>
<td style="text-align: right;">70</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: right;">60</td>
<td style="text-align: right;">81</td>
<td style="text-align: right;">15</td>
</tr>
<tr>
<td style="text-align: right;">70</td>
<td style="text-align: right;">93</td>
<td style="text-align: right;">27</td>
</tr>
<tr>
<td style="text-align: right;">80</td>
<td style="text-align: right;">99</td>
<td style="text-align: right;">43</td>
</tr>
<tr>
<td style="text-align: right;">90</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">64</td>
</tr>
<tr>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">148</td>
</tr>
</tbody>
</table>
<p>Here if the delta to “dumb” is zero or less, then EZR’s trees do no
better than dumb guessing. Note that “dumb” dies surprisingly well: 30%
of the time, just bumbling around looking at five things does as good as
anything else. The success of “dumb” speaks volumes on the current
obsession with big AI. Sure, sometimes we need very complex solutions.
But in a surprisingly large number of cases, dumb old guessing does very
well.</p>
<p>That said, we should not use “dumb”:</p>
<ul>
<li>EZR’s trees often do much better than dumb (see the large number of
positive deltas).</li>
<li>With the dumb method, there is no generalizing. From five randomly
selected rows, it is hard to learn any generalization about the
domain.</li>
<li>On the other hand, EZR does not just make recommendations. It also
returns a tree describing how those recommendations are generated. That
is, EZR lets other people audit or critique (or even complain) about how
decisions are being made.</li>
</ul>
<p>It might be argued that “dumb” is preferred to EZR since it it so
simple. That is perhaps not the strongest argument. As shown below, EZR
is not complex code (just a few hundred lines). Also, it runs very fast.
The entire experiment described about (10 trials over 110 data sets)
took just 65 seconds (on a 10 core mac mini with no GPUs and only 16GB
of memory.).</p>
<h3 id="and-what-does-all-this-tell-us">And what does all this tell
us?</h3>
<p>That, sometimes, complexity is unnecessary. With small tools, small
data, and smart strategies, we can solve real problems efficiently.</p>
<p><strong>EZR</strong> demonstrates how to teach and practice software
engineering grounded in:</p>
<ul>
<li>Simplicity<br />
</li>
<li>Critique<br />
</li>
<li>Ownership</li>
</ul>
<p>all without sacrificing performance.</p>
<h1 id="easer-ai-why">Easer AI: Why?</h1>
<p>This section offers motivation for exploring little AI tools like
EZR.</p>
<h2 id="config-is-a-problem">Config is a problem</h2>
<p>asdas</p>
<h2 id="config-is-a-very-geenratl-problem.">#Config is a very geenratl
problem.</h2>
<p>HBO and icsmn ’24</p>
<h3 id="learning-about-ai">Learning About AI</h3>
<p>If we can make AI simpler, then we can make also simplify the
teaching of AI.</p>
<p>EZR is an interesting candidate for study, for the following
reasons:</p>
<ul>
<li>its system requirements are so low, it can run on system that are
already available to all of us;</li>
<li>it is compact and accessible;</li>
<li>it provides an extensive set of very usable facilities;</li>
<li>it is intrinsically interesting, and in fact breaks new ground in a
number of areas.</li>
</ul>
<p>Not least amongst the charms and virtues of EZR is the compactness of
its source code: in just a few hundred ines of code including tools for
clustering, classification, regression, optimization, explanation,
active learning, statistical analysis, documentation, and test-driven
development.</p>
<p>Such a short code listing is important. For <strong>industrial
practitioners:</strong>, short code examples are easier to understand,
adapt, test, maintain and (if required), port to different languages.
Another reason to explore short code solutions are the security
implications associated with building systems based on long supply
chains. To say the least, it is prudent to replace long supply chains
with tiny local stubs.</p>
<p>Also, for <strong>teaching (or self-study)</strong>, it has often
been suggested that 1,000 lines of code represents the practical limit
in size for a program which is to be understood and maintained by a
single individual<a href="#fn25" class="footnote-ref" id="fnref25"
role="doc-noteref"><sup>25</sup></a>. Most AI tools either exceed this
limit by two orders of magnitude, or else offer the user a very limited
set of facilities, i.e. either the details of the system are
inaccessible to all but the most determined, dedicated and
long-suffering student, or else the system is rather specialised and of
little intrinsic interest.</p>
<p>In my view, it is highly beneficial for anyone studying SE, AI, or
computer science to have the opportunity to study a working AI tool in
all its aspects. Moreover it is undoubtedly good for students majoring
in Computer Science, to be confronted at least once in their careers,
with the task of reading and understanding a program of major
dimensions.</p>
<p>It is my hope that this doc will be of interest and value to students
and practitioners of AI. Although not prepared primarily for use as a
reference work, some will wish to use it as such. For those people, this
code comes with extensive digressions on how parts of it illustrate
various aspects of SE, AI, or computer science.</p>
<h2 id="coding-style">Coding Style</h2>
<h3 id="no-oo">No OO</h3>
<p>No OO. hatton.</p>
<h3 id="dry">DRY</h3>
<p>docu</p>
<h3 id="tdd">TDD</h3>
<h3 id="min-loc.-keep-readability">Min LOC. Keep readability</h3>
<h4 id="functional">Functional</h4>
<h4 id="ternary">Ternary</h4>
<h4 id="auto-typing">Auto-typing</h4>
<h4 id="comprehensions">Comprehensions</h4>
<h3 id="dsl">DSL</h3>
<p>Rule of three</p>
<p>Accordingly, EZR.py usies active learnng to build models froma very
small amunt of dat. Its work can be sumamrised as A-B-C.</p>
<ul>
<li><strong>A</strong>: Use <strong>a</strong>ny examples</li>
<li><strong>B</strong>: <strong>B</strong>uild a model</li>
<li><strong>C</strong>: <strong>C</strong>heck the model</li>
</ul>
<p>EZR supports not just the code but allso the statsitical functions
that lets analst make clear concluios about (e.g.) what kinds of
clustering leads to better conclusions, sooner. With this it…</p>
<p>Teaching . illustrates much of what is missing in current programmer
and sE ltierature (oatterns of productinve coding, isuess of
documentation, encapultion test drivend evelopment etc). It can also be
used a minimal AI teaching toolkit that indotruces students to
clustering. Bayes inference, classfication, rule earling, tree elarning
as well as the stats required to devalauted which of these tools is best
for some current data/</p>
<h2 id="motivation">Motivation</h2>
<h3 id="should-make-it-simpler">Should make it simpler</h3>
<h3 id="can-make-i-simpler">Can make i simpler</h3>
<p>EZR was motivated by the current industrial obsession on Big AI that
seems to be forgetting centuries of experience with data mining. As far
back as 1901, Pearson<a href="#fn26" class="footnote-ref" id="fnref26"
role="doc-noteref"><sup>26</sup></a> showed that tables of data with
<span class="math inline"><em>N</em></span> columns can be modeled with
far fewer columns (where the latter are derived from the eigenvectors of
a correlation information).</p>
<p>Decades of subsequent work has shown that effective models can be
built from data that cover tiny fractions of the possible data space<a
href="#fn27" class="footnote-ref" id="fnref27"
role="doc-noteref"><sup>27</sup></a>. Levnina and Biclet cwnote that</p>
<blockquote>
<p>“The only reason any (learning) methods work … is that, in fact, the
data are not truly high-dimensional. Rather, they are .. can be
efficiently summarized in a space of a much lower dimension.</p>
</blockquote>
<p>(This remarks echoes an early conclusion from Johnson and
Lindenstrauss <a href="#fn28" class="footnote-ref" id="fnref28"
role="doc-noteref"><sup>28</sup></a>.).</p>
<p>For example:</p>
<ul>
<li><strong>Many rows can be ignored</strong>: Data sets with thousands
of rows can be modeled with just a few dozen samples<a href="#fn29"
class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a>.
To explain this, suppose we only want to use models that are well
supported by the data; i.e. supported by multiple rows in a table of
data. This means that many rows in a table can be be replaced by a
smaller number of exemplars.</li>
<li><strong>Many columns can be ignored</strong>: High-dimensional
tables (with many colummns) can be projected into lower dimensional
tables while nearly preserving all pairwise distances<a href="#fn30"
class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a>.
This means that data sets with many columns can be modeled with
surprisingly few columns. e.g. A table of (say) of <span
class="math inline"><em>C</em> = 20</span> columns of binary variables
have a total data space of <span
class="math inline">2<sup>20</sup></span> (which is more than a
million). Yet with just dozens to hundred rows of training data, it is
often possible to build predictors from test rows from that data space.
This is only possible if the signal in this data condenses to a small
regions within the total data space.</li>
<li>Researchers in semi-supervised learning note that high-dimensional
data often lies on a simpler, lower-dimensional ”manifold” embedded
within that higher space <a href="#fn31" class="footnote-ref"
id="fnref31" role="doc-noteref"><sup>31</sup></a>.</li>
</ul>
<p>Numerous AI researchers studying NP-hard tasks report the existence
of a small number of key variables that determine the behavior of the
rest of the model. When such keys are present, then the problem of
controlling an entire model simplifies to just the problem of
controlling the keys.</p>
<p>Keys have been discovered in AI many times and called many different
names: Variable subset selection, narrows, master variables, and
backdoors. In the 1960s, Amarel observed that search problems contain
narrows; i.e. tiny sets of variable settings that must be used in any
solution<a href="#fn32" class="footnote-ref" id="fnref32"
role="doc-noteref"><sup>32</sup></a>. Amarel’s work defined macros that
encode paths between the narrows in the search space, effectively
permitting a search engine to leap quickly from one narrow to
another.</p>
<p>In later work, data mining researchers in the 1990s explored and
examined what happens when a data miner deliberately ignores some of the
variables in the training data. Kohavi and John report trials of data
sets where up to 80% of the variables can be ignored without degrading
classification accuracy<a href="#fn33" class="footnote-ref" id="fnref33"
role="doc-noteref"><sup>33</sup></a>. Note the similarity with Amarel’s
work: it is more important to reason about a small set of important
variables than about all the variables. At the same time, researchers in
constraint satisfaction found “random search with retries” was a very
effective strategy.</p>
<p>Crawford and Baker reported that such searches took less time than a
complete search to find more solutions using just a small number of
retries<a href="#fn34" class="footnote-ref" id="fnref34"
role="doc-noteref"><sup>34</sup></a>. Their ISAMP “iterative sampler”
makes random choices within a model until it gets “stuck”; i.e. until
further choices do not satisfy expectations. When “stuck”, ISAMP does
not waste time fiddling with current choices (as was done by older
chronological backtracking algorithms). Instead, ISAMP logs what
decisions were made before getting “stuck”. It then performs a “retry”;
i.e. resets and starts again, this time making other random choices to
explore.</p>
<p>Crawford and Baker explain the success of this strange approach by
assuming models contain a small set of master variables that set the
remaining variables (and this paper calls such master variables keys).
Rigorously searching through all variable settings is not recommended
when master variables are present, since only a small number of those
settings actually matter. Further, when the master variables are spread
thinly over the entire model, it makes no sense to carefully explore all
parts of the model since much time will be wasted “walking” between the
far-flung master variables. For such models, if the reasoning gets stuck
in one region, then the best thing to do is to leap at random to some
distant part of the model.</p>
<p>A similar conclusion comes from the work of Williams et al.<a
href="#fn35" class="footnote-ref" id="fnref35"
role="doc-noteref"><sup>35</sup></a>. They found that if a randomized
search is repeated many times, that a small number of variable settings
were shared by all solutions. They also found that if they set those
variables before conducting the rest of the search, then formerly
exponential runtimes collapsed to low-order polynomial time. They called
these shared variables the backdoor to reducing computational
complexity.</p>
<p>Combining the above, we propose the following strategy for faster
reasoning about RE models. First, use random search with retries to find
the “key” decisions in RE models. Second, have stakeholders debate, and
then decide, about the keys before exploring anything else. Third, to
avoid trivially small solutions, our random search should strive to
cover much of the model. Code:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Data(src):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> _guess(row):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(interpolate(data,row,<span class="op">*</span>pole) <span class="cf">for</span> pole <span class="kw">in</span> poles)<span class="op">/</span><span class="bu">len</span>(poles)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  head, <span class="op">*</span>rows <span class="op">=</span> <span class="bu">list</span>(src)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  data  <span class="op">=</span> _data(head, rows)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  poles <span class="op">=</span> projections(data)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> row <span class="kw">in</span> rows: row[<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> _guess(row)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> data</span></code></pre></div>
<h3 id="data">Data</h3>
<p>Shared datasets from research papers by Apel [2], Chen <a
href="#fn36" class="footnote-ref" id="fnref36"
role="doc-noteref"><sup>36</sup></a>, and Menzies [^nair] are often used
as case studies of optimization in SE research papers. Chen and Menzies
are collaborating to curate the MOOT repository (Multi-Objective
Optimization Testing4) which offers datasets from recent SE optimization
papers for process tuning, DB configuration, HPO, management decision
making etc.</p>
<p>Since our focus is on configuration, we use MOOT data related to that
task (see Table I and II). Fig. 3 shows the typical structure of those
MOOT data sets. The goal in this data is to tune Spout wait, Spliters,
Counters in order to achieve the best Throughput/Latency. In
summary:</p>
<ul>
<li>MOOT datasets are tables with x inputs and y goals.</li>
<li>The first row shows the column names.</li>
<li>Numeric columns start with uppercase, all others are /symbolic.</li>
<li>Goal columns (e.g. Fig. 3’s Throughput+, Latency-) use +/- to denote
maximize and minimize.</li>
</ul>
<p>Data:</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> independent values          <span class="op">|</span> y <span class="op">=</span> dependent values</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="op">--------------------------------|----------------------</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  Spout_wait, Spliters, Counters, <span class="op">|</span> Throughput<span class="op">+</span>, Latency<span class="op">-</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>     <span class="dv">10</span>,        <span class="dv">6</span>,       <span class="dv">17</span>,      <span class="op">|</span>    <span class="dv">23075</span>,    <span class="fl">158.68</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>      <span class="dv">8</span>,        <span class="dv">6</span>,       <span class="dv">17</span>,      <span class="op">|</span>    <span class="dv">22887</span>,    <span class="fl">172.74</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>      <span class="dv">9</span>,        <span class="dv">6</span>,       <span class="dv">17</span>,      <span class="op">|</span>    <span class="dv">22799</span>,    <span class="fl">156.83</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>      <span class="dv">9</span>,        <span class="dv">3</span>,       <span class="dv">17</span>,      <span class="op">|</span>    <span class="dv">22430</span>,    <span class="fl">160.14</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    ...,      ...,      ...,           ...,    ...</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>  <span class="dv">10000</span>,        <span class="dv">1</span>,       <span class="dv">10</span>,      <span class="op">|</span>   <span class="fl">460.81</span>,    <span class="fl">8761.6</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>  <span class="dv">10000</span>,        <span class="dv">1</span>,       <span class="dv">18</span>,      <span class="op">|</span>   <span class="fl">402.53</span>,    <span class="fl">8797.5</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>  <span class="dv">10000</span>,        <span class="dv">1</span>,       <span class="dv">12</span>,      <span class="op">|</span>   <span class="fl">365.07</span>,    <span class="fl">9098.9</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>  <span class="dv">10000</span>,        <span class="dv">1</span>,        <span class="dv">1</span>,      <span class="op">|</span>   <span class="fl">310.06</span>,    <span class="dv">9421</span></span></code></pre></div>
<p>Note that our data is much larger than the Table 3 example. The 39
data sets in Table I have up to 86,000 rows, 88 independent variables,
and three y goals. For the purposes of illustration, the rows in Table 3
are sorted from best to worst based on those goals. During
experimentation, row order should initially be randomized.</p>
<p>For the purposes of evaluation, all rows in MOOT data sets contain
all their y values. When evaluating the outcome of an optimizer, these
values are used to determine how well the optimizer found the best
rows.</p>
<p>For the purposes of optimization experiments, researchers should hide
the y-values from the optimizer. Each time the optimizer requests the
value of a particular row, this “costs” one unit. For reasons described
below, good optimizers find good goals at least cost (i.e. fewest
labels).</p>
<p>Notes from ase aper</p>
<h2 id="rq5-lmits">RQ5: Lmits</h2>
<p>not generation.</p>
<p>Tabular data</p>
<h2 id="references">References</h2>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>A.G. Bell 2021, The Untold Story of SQLite With Richard
Hipp Corecursive podcast. Interview by Adam Gordon Bell. November 30,
2020. Accessed July 19, 2025.
https://corecursive.com/066-sqlite-with-richard-hipp/<a href="#fnref1"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>X. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, L. Li, X.
Luo, D. Lo, J. Grundy, and H. Wang, “Large language models for software
engineering: A systematic literature review,” ACM Trans. Softw. Eng.
Methodol., vol. 33, no. 8, Dec. 2024. [Online]. Available:
https://doi.org/10.1145/3695988<a href="#fnref2" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>X. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, L. Li, X.
Luo, D. Lo, J. Grundy, and H. Wang, “Large language models for software
engineering: A systematic literature review,” ACM Trans. Softw. Eng.
Methodol., vol. 33, no. 8, Dec. 2024. [Online]. Available:
https://doi.org/10.1145/3695988<a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>W. Fu and T. Menzies, “Easy over hard: a case study on
deep learning,” in Proceedings of the 2017 11th Joint Meeting on
Foundations of Software Engineering, ser. ESEC/FSE 2017. New York, NY,
USA: Association for Computing Machinery, 2017, p. 49–60. [Online].
Available: https://doi.org/10.1145/3106237. 3106256<a href="#fnref4"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>L. Grinsztajn, E. Oyallon, and G. Varoquaux, “Why do
tree-based models still outperform deep learning on typical tabular
data?” in NeurIPS’22, 2022.<a href="#fnref5" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>X. Ling, T. Menzies, C. Hazard, J. Shu, and J. Beel,
“Trading off scalability, privacy, and performance in data synthesis,”
IEEE Access, vol. 12, pp. 26 642–26 654, 2024.<a href="#fnref6"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>S. Majumder, N. Balaji, K. Brey, W. Fu, and T. Menzies,
“500+ times faster than deep learning,” in Proceedings of the 15th
International Conference on Mining Software Repositories. ACM, 2018.<a
href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>S. Somvanshi, S. Das, S. A. Javed, G. Antariksa, and A.
Hossain, “A survey on deep tabular learning,” arXiv preprint
arXiv:2410.12034, 2024.<a href="#fnref8" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>V. Tawosi, R. Moussa, and F. Sarro, “Agile effort
estimation: Have we solved the problem yet? insights from a replication
study,” IEEE Transactions on Software Engineering, vol. 49, no. 4,
pp. 2677– 2697, 2023.<a href="#fnref9" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>T. Menzies, “Retrospective: Data Mining Static Code
Attributes to Learn Defect Predictors” in IEEE Transactions on Software
Engineering, vol. 51, no. 03, pp. 858-863, March 2025, doi:
10.1109/TSE.2025.3537406.<a href="#fnref10" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Settles, Burr. “Active learning literature survey.”
(2009).<a href="#fnref11" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>T. Menzies and T. Chen, MOOT repository of
Multi-objective optimization tests. 2025. <a
href="http://github.com/timm/moot">http://github.com/timm/moot</a><a
href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>S. Amarel, “Program synthesis as a theory formation
task: problem representations and solution methods,” in Machine
Learning: An Artificial Intelligence Approach. Morgan Kaufmann, 1986.<a
href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>W. B. Johnson and J. Lindenstrauss, “Extensions of
lipschitz mappings into a hilbert space,” Contemporary Mathematics,
vol. 26, pp. 189–206, 1984.<a href="#fnref14" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>J. M. Crawford and A. B. Baker, “Experimental results
on the application of satisfiability algorithms to scheduling problems,”
in Proceedings of the Twelfth National Conference on Artificial
Intelligence (Vol. 2), Menlo Park, CA, USA, 1994, pp. 1092–1097.<a
href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>R. Kohavi and G. H. John, “Wrappers for feature subset
selection,” Artif. Intell., vol. 97, no. 1-2, pp. 273–324, Dec. 1997.<a
href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>R. Williams, C. P. Gomes, and B. Selman, “Backdoors to
typical case complexity,” in Proceedings of the International Joint
Conference on Artificial Intelligence, 2003.<a href="#fnref17"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>D. Zhou et al., “Learning with Local and Global
Consistency”, 2005.<a href="#fnref18" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>T. Menzies, “The Few Key Rows”, 2008 (or your actual
source).<a href="#fnref19" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>B. Settles, “Active Learning Literature Survey”,
2009.<a href="#fnref20" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p>Tu, Huy, Zhe Yu, and Tim Menzies. “Better data
labelling with emblem (and how that impacts defect prediction).”
<em>IEEE TSE</em>, 48.1 (2020): 2<a href="#fnref21"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>Zhe Yu, Fahmid Morshed Fahid, Huy Tu, and Tim Menzies.
Identifying self-admitted technical debts with jitterbug: A two-step
approach. IEEE Transactions on Software Engineering, 48(5):1676–1691,
2022.<a href="#fnref22" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p>Xiaoxue Wu, Wei Zheng, Xin Xia, and David Lo. Data
quality matters: A case study on data label correctness for security bug
report prediction. IEEE Transactions on Software Engineering,
48(7):2541–2556, 2022.<a href="#fnref23" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p>Toufique Ahmed, Premkumar Devanbu, Christoph Treude,
and Michael Pradel. Can LLMs replace manual annotation of software
engineering artifacts? In MSR’25, 2025<a href="#fnref24"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><p>Lions, John (1996). Lions’ Commentary on UNIX 6th
Edition with Source Code. Peer-to-Peer Communications. ISBN
978-1-57398-013-5.<a href="#fnref25" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn26"><p>Pearson, K. (1901). “On Lines and Planes of Closest Fit
to Systems of Points in Space”. Philosophical Magazine. 2 (11): 559–572.
10.1080/14786440109462720.<a href="#fnref26" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn27"><p>I. Witten, E. Frank, and M. Hall. Data Mining:
Practical Machine Learning Tools and Techniques Morgan Kaufmann Series
in Data Management Systems Morgan Kaufmann, Amsterdam, 3 edition,
(2011)<a href="#fnref27" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn28"><p>W. B. Johnson and J. Lindenstrauss, “Extensions of
lipschitz mappings into a hilbert space,” Contemporary Mathematics,
vol. 26, pp. 189–206, 1984.<a href="#fnref28" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn29"><p>T. Menzies, “The Few Key Rows”, 2008 (or your actual
source).<a href="#fnref29" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn30"><p>W. B. Johnson and J. Lindenstrauss, “Extensions of
lipschitz mappings into a hilbert space,” Contemporary Mathematics,
vol. 26, pp. 189–206, 1984.<a href="#fnref30" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn31"><p>D. Zhou et al., “Learning with Local and Global
Consistency”, 2005.<a href="#fnref31" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn32"><p>S. Amarel, “Program synthesis as a theory formation
task: problem representations and solution methods,” in Machine
Learning: An Artificial Intelligence Approach. Morgan Kaufmann, 1986.<a
href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn33"><p>R. Kohavi and G. H. John, “Wrappers for feature subset
selection,” Artif. Intell., vol. 97, no. 1-2, pp. 273–324, Dec. 1997.<a
href="#fnref33" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn34"><p>J. M. Crawford and A. B. Baker, “Experimental results
on the application of satisfiability algorithms to scheduling problems,”
in Proceedings of the Twelfth National Conference on Artificial
Intelligence (Vol. 2), Menlo Park, CA, USA, 1994, pp. 1092–1097.<a
href="#fnref34" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn35"><p>R. Williams, C. P. Gomes, and B. Selman, “Backdoors to
typical case complexity,” in Proceedings of the International Joint
Conference on Artificial Intelligence, 2003.<a href="#fnref35"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn36"><p>M. Li, T. Chen, and X. Yao, “How to evaluate solutions
in pareto-based search-based software engineering: A critical review and
methodological guidance,” IEEE Transactions on Software Engineering,
vol. 48, no. 5, pp. 1771–1799, 2022<a href="#fnref36"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
